\documentclass[11pt,onecolumn]{IEEEtran}
\input{preamble.tex}

%\usepackage{amsmath,amssymb,amsthm}
%\usepackage{geometry}
%\usepackage{hyperref}
%\usepackage{txfonts}
%\usepackage{concmath}
%\usepackage{eulervm}
%\geometry{margin=1in}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}

\newcommand{\K}{K}
\newcommand{\Hamming}{\mathrm{H}}

\title{Why Evolution Operates Near One Mutation per Genome per Generation}
\author{Chattopadhyay}
\date{\today}

\begin{document}
\maketitle


\begin{abstract}
Across organisms and viruses, the per-site mutation rate scales inversely with genome length, yielding an approximately constant number of mutations per genome per generation (Drake’s rule). Existing explanations emphasize biochemical fidelity constraints or population-genetic error-threshold arguments that limit mutation from above. Here we provide a complementary, mechanism-independent account based on variation supply. Treating mutation as a blind local perturbation process, we ask which mutation rates maximize the expected rate of producing variants that are atypical relative to the mutation kernel itself, and therefore available for selective amplification rather than drift into mutational background. Using counting bounds for the fraction of such mutation-atypical outcomes within Hamming neighborhoods and combining them with an independent per-site mutation model, we obtain an explicit discovery-rate expression that is maximized when the per-genome mutation intensity $T = n\mu$ is $O(1)$, with a peak near $T \approx 1$. Consequently, the optimal per-site rate scales as $\mu^\star = (1 + o(1))/n$. These results derive Drake’s rule from a generic supply--destruction tradeoff under local stochastic perturbations, complementing fidelity- and stability-based perspectives.
\end{abstract}





Mutation injects randomness; selection preserves and amplifies structure. Evolution therefore operates under a fundamental tension between entropy production and entropy suppression. A striking empirical regularity across diverse taxa is that the per-site mutation rate $\mu$ scales approximately as $1/n$, where $n$ is genome length, so the per-genome mutation rate $U=n\mu$ is roughly constant. This phenomenon, observed from RNA viruses to eukaryotes, is known as Drake’s rule~\cite{Drake1991,Lynch2010,SniegowskiRaynes2013,Hershberg2015}.

Standard explanations of Drake’s rule fall into two broad categories. One emphasizes biochemical constraints and tradeoffs in replication fidelity, proofreading, and energetic cost~\cite{Lynch2010}. The other emphasizes the population-genetic consequences of excessive mutation, most prominently Eigen’s error-threshold framework~\cite{Eigen1971}, in which high mutation loads destabilize inherited information and limit the maintenance of adapted structure. These approaches provide important insight and useful upper bounds, but they primarily explain why mutation rates cannot be too large. They do not, by themselves, explain why evolution repeatedly operates near a particular scaling regime with $U=O(1)$.

Here we pursue a complementary route that is upstream of selection and does not invoke biochemical detail, explicit fitness landscapes, or equilibrium population genetics. We ask: given a blind local mutation mechanism, at what mutation load is the \emph{supply rate} of statistically exceptional variants maximized? The point is not that evolution seeks exceptionality per se, but that selection can only act on what mutation supplies. If mutation predominantly produces outcomes that are typical under its own kernel, then repeated mutation pushes lineages toward the kernel's typical set, and accumulated organization is continually eroded into mutational background. Any sustained adaptive process therefore requires a nonzero supply of outcomes that are atypical relative to mutation alone, since these are the only candidates that can be preferentially retained rather than washed out.

Our starting observation is that mutation induces a natural statistical model for an offspring relative to its parent. For a parent genome $x\in[q]^n$, an $m$-mutation restricts the offspring $y$ to the Hamming sphere $H_m(x)$, the set of sequences at Hamming distance $m$ from $x$. Under the mutation-induced model that is uniform over $H_m(x)$, a typical draw requires $\log|H_m(x)|$ bits to specify once the radius is fixed, and
\[
|H_m(x)|=\binom{n}{m}(q-1)^m
\]
grows rapidly with $m$. This immediately implies a notion of typicality: at a given radius, most outcomes resemble typical draws from the mutation kernel, and only a small fraction are exceptional relative to that kernel. In particular, the larger the neighborhood, the thinner the tail of mutation-atypical outcomes within it.

We formalize exceptionality relative to the mutation-induced model on $H_m(x)$ using randomness deficiency (Definition~S1 in the SI Appendix), which measures how much rarer an outcome is than a typical draw from the same model. A two-part coding/counting argument (Theorem~S1 and Lemma~S1) implies that, for typical parents, the fraction of $\Delta$-exceptional outcomes at radius $m$ scales as $2^{-\Delta}/|H_m(x)|$ up to polylogarithmic factors. We then combine this typical-case rarity with an independent per-site mutation model $M\sim\mathrm{Binomial}(n,\mu)$ to define an expected discovery rate $\Phi(\mu)$ that averages the $\Delta$-exceptional probability over mutation radii (Eq.~S14). Optimizing $\Phi(\mu)$ yields an interior optimum at per-genome mutation intensity $T=n\mu=O(1)$ (Theorem~S2), and in particular,
\[
\mu^\star=\frac{1+o(1)}{n}.
\]
Thus Drake’s rule emerges as a mechanism-independent supply optimum under blind local perturbation, complementing fidelity- and stability-based perspectives such as the error-threshold viewpoint.











%% \section*{Introduction}

%% Mutation injects randomness; selection preserves and amplifies structure. Evolution therefore operates under a fundamental tension between entropy production and entropy suppression. A striking empirical regularity across diverse taxa is that the per-site mutation rate $\mu$ scales approximately as $1/n$, where $n$ is genome length, so the per-genome mutation rate $U=n\mu$ is roughly constant. This phenomenon, observed from RNA viruses to eukaryotes, is known as Drake’s rule~\cite{Drake1991,Lynch2010,SniegowskiRaynes2013,Hershberg2015}.

%% Standard explanations of Drake’s rule fall into two broad categories. One emphasizes biochemical constraints and tradeoffs in replication fidelity, proofreading, and energetic cost~\cite{Lynch2010}. The other emphasizes the population-genetic consequences of excessive mutation, most prominently Eigen’s error-threshold framework~\cite{Eigen1971}, in which high mutation loads destabilize inherited information and limit the maintenance of adapted structure. These approaches provide important insight and useful upper bounds, but they primarily explain why mutation rates cannot be too large. They do not, by themselves, explain why evolution repeatedly operates near a particular scaling regime with $U=O(1)$.

%% Here we pursue a complementary route that is upstream of selection and does not invoke biochemical detail, explicit fitness landscapes, or equilibrium population genetics. We ask: given a blind local mutation mechanism, at what mutation load is the \emph{supply rate} of statistically exceptional variants maximized? The point is not that evolution seeks exceptionality per se, but that selection can only act on what mutation supplies; without a continuing supply of mutation-atypical outcomes, repeated perturbation would drive lineages toward the typical set of the mutation kernel.

%% Our starting observation is that mutation induces a natural statistical model for an offspring relative to its parent. For a parent genome $x\in[q]^n$, an $m$-mutation restricts the offspring $y$ to the Hamming sphere $H_m(x)$, the set of sequences at Hamming distance $m$ from $x$. Under the mutation-induced model that is uniform over $H_m(x)$, a typical draw requires $\log|H_m(x)|$ bits to specify once the radius is fixed, and
%% \[
%% |H_m(x)|=\binom{n}{m}(q-1)^m
%% \]
%% grows rapidly with $m$. This immediately implies a notion of typicality: at a given radius, most outcomes resemble typical draws from the mutation kernel, and only a small fraction are exceptional relative to that kernel.

%% We formalize exceptionality relative to the mutation-induced model on $H_m(x)$ using randomness deficiency (Definition~S1 in the SI Appendix). A two-part coding/counting argument (Theorem~S1 and Lemma~S1) implies that, for typical parents, the fraction of $\Delta$-exceptional outcomes at radius $m$ scales as $2^{-\Delta}/|H_m(x)|$ up to polylogarithmic factors. We then combine this typical-case rarity with an independent per-site mutation model $M\sim\mathrm{Binomial}(n,\mu)$ to define an expected discovery rate $\Phi(\mu)$ that averages the $\Delta$-exceptional probability over mutation radii (Eq.~S14). Optimizing $\Phi(\mu)$ yields an interior optimum at per-genome mutation intensity $T=n\mu=O(1)$ (Theorem~S2), and in particular,
%% \[
%% \mu^\star=\frac{1+o(1)}{n}.
%% \]
%% Thus Drake’s rule emerges as a mechanism-independent supply optimum under blind local perturbation, complementing fidelity- and stability-based perspectives such as the error-threshold viewpoint.







\section*{Discussion}

This work provides a mechanism-independent route to Drake's rule from the standpoint of variation supply. Under a minimal model in which mutation acts as a local stochastic perturbation on a discrete sequence space, we derived a typical-case bound on the probability that an $m$-mutant is statistically exceptional relative to its mutation neighborhood. Combining this rarity scaling with an independent per-site mutation model yields an explicit expression for the expected rate at which mutation supplies such exceptional variants, and shows that this rate is maximized when the expected number of mutations per genome per generation is $O(1)$. In the usual per-site parametrization, this corresponds to the inverse-length scaling $\mu^\star=\Theta(1/n)$.

A useful way to express the optimum is through the per-genome mutation intensity
\[
T \;:=\; n\mu \;=\; \mathbb{E}[M],\qquad M\sim{\rm Binomial}(n,\mu),
\]
which plays the role of an intensive ``mutation temperature'' controlling the typical perturbation size. Figure~\ref{figsim} illustrates the scaling directly. In the left panel, $\Phi(\mu)$ plotted against the per-site rate $\mu$ exhibits a single interior maximum whose location shifts left as $n$ increases, consistent with $\mu^\star\propto 1/n$. In the right panel, plotting the same discovery rate against $T=n\mu$ collapses the curves for different $n$ onto a common profile with a maximum near $T\approx 1$, matching the predicted $T e^{-T}$ dependence up to a multiplicative constant. The collapse emphasizes that the scaling law is more naturally stated as a constant optimal per-genome perturbation size.

Our notion of discovery is defined relative to the mutation-induced model. We do not claim that the Hamming sphere is the globally optimal statistical model for an offspring, only that it is the model naturally induced by the mutation mechanism. Since mutation is the sole upstream source of variation, selection can only act on what mutation supplies. In particular, sustained accumulation of biological organization requires a nonzero supply of variants that are atypical relative to mutation alone. If offspring were always typical draws from the mutation kernel, then over many generations the population would be driven toward the typical set of that kernel, and previously accumulated structure would be eroded. Positive randomness deficiency provides a quantitative, mechanism-independent way to express this minimal sense in which a variant is not merely ``background'' mutational noise.

The inverse-length scaling is robust to the choice of novelty margin $\Delta$, provided $\Delta$ does not grow with $n$ fast enough to dominate the one-step indexing cost $c(1)=\Theta(\log n)$. In particular, any fixed $\Delta$ or $\Delta=o(\log n)$ yields the same $\mu^\star=\Theta(1/n)$ scaling, with $\Delta$ affecting only the overall rate through the factor $2^{-\Delta}$.


%We do not claim that natural selection explicitly maximizes this objective, nor do we model fitness effects. The discovery rate isolates an upstream constraint imposed by mutation itself: mutation generates variants blindly, and only those that are atypical relative to the mutation kernel can, in principle, be preferentially amplified. Optimizing this supply rate therefore identifies a structural regime of mutation independent of any specific fitness landscape.


\begin{figure}[t]

  \includegraphics[width=.95\textwidth]{code/temperature_scaling_sim.pdf}
  
\caption{
Scaling and collapse of the discovery rate under independent per-site mutation.
\textbf{Left:} The expected discovery rate $\Phi(\mu)$ as a function of the per-site mutation rate $\mu$ for increasing genome lengths $n$ (theory and Monte Carlo simulation). The location of the maximum shifts left as $n$ increases, consistent with the inverse-length scaling $\mu^\star \propto 1/n$.
\textbf{Right:} The same discovery rate plotted against the per-genome mutation intensity $T = n\mu$. Curves for different $n$ collapse onto a common profile with a maximum near $T \approx 1$, matching the predicted $T e^{-T}$ dependence up to a multiplicative constant. This collapse demonstrates that the optimal regime corresponds to a constant $O(1)$ number of mutations per genome per generation.
}\label{figsim}
\end{figure}


\subsection*{Why focus on discovery probability?}

A natural objection is to ask why evolution should be viewed as optimizing the probability of structure discovery, rather than a more direct quantity such as expected fitness gain~\cite{Lynch2010,SniegowskiRaynes2013,Hershberg2015}. We emphasize that our goal is not to model fitness, nor to assert that natural selection explicitly maximizes the objective we write down. Rather, we use discovery probability to isolate an upstream constraint imposed by mutation itself on the supply of potentially selectable variants.

Fitness effects of mutations are inherently unknown \emph{a priori}. From the perspective of the mutation process, there is no access to a fitness landscape. Mutation generates variants blindly, and selection subsequently amplifies or suppresses them. In this setting, one can ask a counterfactual but well-posed question: given a local mutation kernel, which per-site mutation rate maximizes the expected rate at which mutation produces variants that are atypical relative to that kernel. Randomness deficiency relative to the mutation-induced model captures precisely this notion of atypicality: such variants are rare under mutation alone and therefore constitute candidates that selection can preferentially amplify if they confer advantage in a given environment.


Finally, it is useful to situate this result relative to classical ``error threshold'' arguments. Eigen-type frameworks constrain mutation from above by requiring that inherited information remain stable under copying errors~\cite{Eigen1971}. Our result is different in kind: it identifies an interior optimum for the upstream \emph{supply} of mutation-atypical variants under blind local perturbations, without invoking a fitness landscape or equilibrium population genetics. The fact that both perspectives emphasize the $U=T=n\mu=O(1)$ regime suggests that Drake's rule may reflect the intersection of two generic pressures: fidelity constraints that limit loss of existing information, and supply constraints that limit how often mutation produces variants sufficiently atypical to be preferentially amplified. In this sense, operating near one mutation per genome per generation can be read as a regime in which both retention and exploration remain nontrivial.


This framing connects naturally to classical biology concepts. Population-genetic error-threshold arguments, such as Eigen's, constrain mutation from above by requiring heritable information to remain stable under copying errors~\cite{Eigen1971}. Our result is different in kind: it identifies an \emph{optimal} scaling for the upstream supply of rare variants under blind local perturbations. The fact that both viewpoints emphasize the $1/n$ regime suggests a convergence of two pressures, fidelity constraints that prevent loss of existing information and supply constraints that prevent evolutionary stasis. In this sense, operating near one mutation per genome per generation can be interpreted as a compromise between maintaining inherited structure and generating a steady trickle of mutation-atypical variants on which selection can act.


\subsection*{Beyond biological specificity}

Although motivated by biological evolution, the argument presented here is not inherently biological. No assumptions are made about molecular replication, detailed population structure, or selection dynamics beyond the minimal requirement that selection acts on variants supplied by mutation. The derivation depends only on three ingredients: a discrete configuration space, a local stochastic perturbation mechanism, and the requirement that adaptive progress requires access to variants that are atypical relative to the perturbation mechanism itself.

In this sense, Drake's rule emerges as a special case of a more general principle governing exploration under constrained generative processes. Any system in which novel structure must be discovered through local random perturbations faces an analogous tradeoff: perturbations that are too small fail to generate exceptionality at an appreciable rate, while perturbations that are too large destroy exploitable structure. The resulting optimum selects a regime in which the expected perturbation size is $O(1)$ relative to system dimension. Figure~\ref{figsim} makes this high-dimensional scaling visible by showing that the optimal regime is characterized by a constant intensive perturbation parameter $T$.

This perspective suggests that the inverse scaling of mutation rate with genome length is not an idiosyncratic feature of biology, but a manifestation of a general information-theoretic constraint on structure discovery under blind local exploration. Biological evolution realizes this constraint through molecular mutation and selection, but the same logic applies to any adaptive process operating under local stochastic perturbations.

In summary, the combinatorial growth of mutation neighborhoods suppresses the probability of mutation-atypical outcomes at large radii, and optimizing the resulting discovery rate under independent per-site mutation yields the observed inverse scaling $\mu^\star \sim 1/n$, equivalently an $O(1)$ mutation load per genome.



\bibliographystyle{IEEEtran}
\bibliography{kevo}

%\end{document}

\clearpage


\section*{Supplementary Methods}
\subsection*{Sequence Space, Mutation Geometry and Two-Part Codes}\label{sec:si:setup}

Let $[q]^n$ denote the set of length-$n$ strings over an alphabet of size $q \ge 2$. For $x,y\in[q]^n$, let $d_{\Hamming}(x,y)$ denote the Hamming distance.
For $m\in\{0,1,\dots,n\}$, define the Hamming sphere
\cgather{
H_m(x)=\{y\in[q]^n : d_{\Hamming}(x,y)=m\}. \label{eq:si:hammingsphere}
}
with cardinality $|H_m(x)|=\binom{n}{m}(q-1)^m$, where $m$ represents the number of mutated sites in one generation.
Let $\K(\cdot)$ denote prefix-free Kolmogorov complexity with respect to a fixed universal Turing machine~\cite{LiVitanyi2019}.
If $x\in S$ and $S$ is a finite set, then the two-part code bound is as follows:
\cgather{
\K(x) \le \K(S)+\log|S|+O(1),
\qquad
\K(x\mid S)\le \log|S|+O(1). \label{eq:si:twopart}
}
which expresses the optimality condition of two-part codes~\cite{LiVitanyi2019,GacsTrompVitanyi2001}: first describe the model $S$, then describe the index of $x$ within $S$.

Next, we note that the mutation process induces a restricted model class. For fixed parent $x$ and mutation radius $m$, the Hamming sphere $H_m(x)$ is the set of sequences accessible by an $m$-mutation. Crucially, $\K(H_m(x)\mid x,m)=O(1)$, which implies
$\K(H_m(x))=\K(x)+O(\log n)$,
where the logarithmic term accounts for encoding $(n,m,q)$.
This leads to the interpretation of \emph{mutation indexing cost} as
\cgather{
c(m)=\log|H_m(x)|=\log\binom{n}{m}+m\log(q-1). \label{eq:si:cm}
}
Thus, random mutation induces the two-part code
\cgather{
\K(y)\;\approx\;\K(x)+c(m), \label{eq:si:twopartmutation}
}
for $y$ uniformly drawn from $H_m(x)$.
Finally, for a model $S\ni y$, the randomness deficiency~\cite{LiVitanyi2019,GacsTrompVitanyi2001} is
\cgather{
\Delta_y(S)=\K(S)+\log|S|-\K(y). \label{eq:si:deficiency}
}
Thus, admissible models are restricted to mutation-accessible sets $H_m(x)$.

\begin{definition}[Net structure discovery]\label{def:si:discovery}
Fix $\Delta\ge 1$. A mutant $y\in H_m(x)$ constitutes a \emph{net structure discovery} if
$\Delta_y\big(H_m(x)\big)\ge\Delta$, or equivalently, $\K(y)\le \K(x)+c(m)-\Delta$.
\end{definition}
Net structure discovery formalizes novelty as being exceptional relative to what mutation alone would typically produce.

\begin{theorem}[Universal upper bound]\label{thm:si:upper}
For any $x\in[q]^n$, any $m$, and any $\Delta\ge 1$,
\cgather{
\Pr_{y\sim \mathrm{Unif}(H_m(x))}\big[\Delta_y(H_m(x))\ge\Delta\big]
\le
O\left(\frac{2^{-\Delta}}{|H_m(x)|}\right). \label{eq:si:upper}
}
\end{theorem}

\begin{IEEEproof}
If $\Delta_y(H_m(x))\ge\Delta$, then
$\K(y)\le \K(H_m(x))+\log|H_m(x)|-\Delta$.
By the Kraft inequality for prefix-free complexity, at most $2^{T+1}$ strings satisfy $\K(y)\le T$. Substituting
$T=\K(H_m(x))+\log|H_m(x)|-\Delta$ and dividing by $|H_m(x)|$ yields the bound.
\end{IEEEproof}

\begin{lemma}[Typical tightness for incompressible parents]\label{lem:si:tight}
Let $x$ satisfy $\K(x)\ge n\log q-O(1)$. Then for $y$ drawn uniformly from $H_m(x)$,
\cgather{
\K(y\mid x,m)=\log|H_m(x)|\pm O(1), \label{eq:si:condK}
}
with probability $1-O(1/|H_m(x)|)$. Consequently, up to polylogarithmic factors,
\cgather{
\Pr\big[\Delta_y(H_m(x))\ge\Delta\big]
=
\Theta\left(\frac{2^{-\Delta}}{|H_m(x)|}\right). \label{eq:si:tight}
}
\end{lemma}

\begin{IEEEproof}
Fix $x$ and $m$. There exists a computable bijection
\cgather{
\pi_{x,m}:\{1,\dots,|H_m(x)|\}\to H_m(x) \label{eq:si:bijection}
}
such that given $(x,m,i)$ one can compute $y=\pi_{x,m}(i)$. Hence
\cgather{
\K(y\mid x,m)\le \log|H_m(x)|+O(1) \label{eq:si:condKup}
}
for all $y\in H_m(x)$. For the lower tail, define
\cgather{
A_\Delta=\{y\in H_m(x):\K(y\mid x,m)\le \log|H_m(x)|-\Delta\}. \label{eq:si:Adef}
}
Each $y\in A_\Delta$ has a prefix-free description of length at most
$\log|H_m(x)|-\Delta$, so $|A_\Delta|\le |H_m(x)|2^{-\Delta+O(1)}$. Thus
\cgather{
\Pr[y\in A_\Delta]\le 2^{-\Delta+O(1)}. \label{eq:si:Atail}
}
Since $x$ is incompressible, the two-part code through $H_m(x)$ is optimal up to $O(\log n)$ terms. Therefore
\cgather{
\Delta_y(H_m(x))
=
\log|H_m(x)|-\K(y\mid x,m)\pm O(\log n), \label{eq:si:deficiencyexpand}
}
which yields the stated asymptotic bound.
\end{IEEEproof}

\begin{cor}[Expected discovery under binomial mutation]\label{cor:si:Phi}
Let $X\sim \mathrm{Unif}([q]^n)$ be the parent sequence.
Let $M\sim \mathrm{Binomial}(n,\mu)$ denote the number of mutated sites,
and conditional on $(X,M=m)$ let $Y\sim \mathrm{Unif}(H_m(X))$. Define the expected discovery probability
\cgather{
\Phi(\mu)
:=
\mathbb{P}\left(\Delta_Y(H_M(X)) \ge \Delta \right)
=
\sum_{m=1}^n
\Pr(M=m)\,
\Pr_{y\sim H_m(X)} \left[\Delta_y(H_m(X)) \ge \Delta \right]. \label{eq:si:PhiDef}
}
Then, for $\Delta=o(\log n)$,
\cgather{
\Phi(\mu)\asymp
2^{-\Delta}
\sum_{m=1}^n
\Pr(M=m)
\frac{1}{|H_m(X)|}. \label{eq:si:PhiAsymp}
}
\end{cor}

\begin{IEEEproof}
For $X\sim \mathrm{Unif}([q]^n)$, the standard incompressibility bound
\cgather{
\Pr\left[K(X)\le n\log q - t\right]\le q^{-t} \label{eq:si:incomp}
}
implies that $X$ satisfies the incompressibility condition of Lemma~\ref{lem:si:tight}
with overwhelming probability. On this event, Lemma~\ref{lem:si:tight} yields
\cgather{
\Pr_{y\sim H_m(X)}
\left[\Delta_y(H_m(X)) \ge \Delta \right]
\asymp
\frac{2^{-\Delta}}{|H_m(X)|}. \label{eq:si:probm}
}
The contribution of the exceptional set $\{X: K(X)\le n\log q - t\}$ is at most $q^{-t}$ and is negligible
for $t=\omega(1)$. Substituting the tight bound into the definition of $\Phi(\mu)$ gives the stated expression.
\end{IEEEproof}

Note that upon substituting the binomial mutation model and the sphere cardinality
$|H_m(X)|=\binom{n}{m}(q-1)^m$, the combinatorial factor $\binom{n}{m}$ appearing in $\Pr(M=m)$ cancels
with the identical factor in $|H_m(X)|$. Thus the multiplicity of mutation-location choices does not amplify
discovery probability: it is already accounted for by the mutation process itself. What remains is the symbol-choice
entropy $(q-1)^m$ and the exponential suppression arising from the rarity of exceptional strings within each
mutation-accessible model. We can therefore state the small-$\mu$ asymptotics of the discovery rate as follows.

\begin{lem}[Small-$\mu$ asymptotics of discovery rate]\label{lem:si:smallmu}
For $\mu=o(1)$,
\cgather{
\sum_{m=1}^n\left(\frac{\mu}{q-1}\right)^m
=
\frac{\mu}{q-1}+O(\mu^2),
\qquad
(1-\mu)^n=e^{-n\mu+O(n\mu^2)}. \label{eq:si:smallmu}
}
Consequently,
\cgather{
\Phi(\mu)\asymp
C\,2^{-\Delta}\,\frac{\mu}{q-1}\,e^{-n\mu+O(n\mu^2)}, \label{eq:si:PhiSmallmu}
}
where $C$ absorbs polylogarithmic factors.
\end{lem}

\begin{IEEEproof}
The first expansion is the truncated geometric series. Substituting
$n\log(1-\mu)=-n\mu+O(n\mu^2)$ into the expression for $\Phi(\mu)$ gives the claim.
\end{IEEEproof}

\begin{theorem}[Optimal mutation rate]\label{thm:si:opt}
In the regime $\mu=o(1)$ with $n\mu^2=o(1)$, the leading term $\mu e^{-n\mu}$ is maximized at $n\mu=1$.
Consequently,
\cgather{
\mu^\star=\frac{1+o(1)}{n}. \label{eq:si:muopt}
}
\end{theorem}

\begin{corollary}[Drake’s rule]\label{cor:si:drake}
Optimizing information-theoretic structure discovery under random mutation yields an inverse scaling of per-site
mutation rate with genome length.
\end{corollary}


\end{document}



%% \section*{Introduction}

%% Mutation injects randomness; selection preserves and amplifies structure. Evolution therefore operates under a fundamental tension between entropy production and entropy suppression. A striking empirical regularity across diverse taxa is that the per-site mutation rate $\mu$ scales approximately as $1/n$, where $n$ is genome length, so the per-genome mutation rate $U=n\mu$ is roughly constant. This phenomenon, observed from RNA viruses to eukaryotes, is known as Drake’s rule~\cite{Drake1991,Lynch2010,SniegowskiRaynes2013,Hershberg2015}.

%% Standard explanations of Drake’s rule fall into two broad categories. One emphasizes biochemical constraints and tradeoffs in replication fidelity, proofreading, and energetic cost~\cite{Lynch2010}. The other emphasizes the population-genetic consequences of excessive mutation, most prominently Eigen’s error-threshold framework~\cite{Eigen1971}, in which high mutation loads destabilize inherited information and limit the maintenance of adapted structure. These approaches provide important insight and useful upper bounds, but they primarily explain why mutation rates cannot be too large. They do not, by themselves, explain why evolution repeatedly operates near a particular scaling regime with $U=O(1)$.

%% Here we pursue a complementary route that is upstream of selection and does not invoke biochemical detail, explicit fitness landscapes, or equilibrium population genetics. We ask: given a blind local mutation mechanism, at what mutation load is the \emph{supply rate} of statistically exceptional variants maximized? The point is not that evolution seeks exceptionality per se, but that selection can only act on what mutation supplies. A sustained adaptive process therefore requires a continuing supply of variants that are not merely generic noise outcomes of the mutation kernel.

%% Our starting observation is that mutation induces a natural statistical model for an offspring relative to its parent. For a parent genome $x\in[q]^n$, an $m$-mutation restricts the offspring $y$ to the Hamming sphere $H_m(x)$, the set of sequences at Hamming distance $m$ from $x$. Under the mutation-induced model that is uniform over $H_m(x)$, a typical draw requires $\log|H_m(x)|$ bits to specify once the radius is fixed, and
%% \[
%% |H_m(x)|=\binom{n}{m}(q-1)^m
%% \]
%% grows rapidly with $m$. This immediately implies a notion of typicality: at a given radius, most outcomes resemble typical draws from the mutation kernel, and only a small fraction are atypical (exceptional) relative to that kernel. \DQS{say this atypicality is needed to avoid descnt into noise at any mutation rate}


%% We formalize atypicality relative to the mutation-induced model on $H_m(x)$ using randomness deficiency, a standard notion in algorithmic statistics (see SI for definitions and proofs). Intuitively, only a vanishing fraction of outcomes in a large discrete model class can be described substantially shorter than the typical two-part code for that class. This counting principle implies that, for typical parents, the fraction of $\Delta$-exceptional mutants at radius $m$ scales as $2^{-\Delta}/|H_m(x)|$ up to polylogarithmic factors.

%% We combine this typical-case rarity with an independent per-site mutation model $M\sim\mathrm{Binomial}(n,\mu)$ to obtain an explicit expected discovery rate $\Phi(\mu)$. Optimizing $\Phi(\mu)$ over $\mu$ yields an interior optimum at per-genome mutation intensity $T=n\mu=O(1)$, and hence $\mu^\star=\Theta(1/n)$.



%% Using this approach, we show that for typical parent sequences (in the above counting sense), the probability that an $m$-mutation produces a $\Delta$-exceptional variant scales as $\Theta(2^{-\Delta}/|H_m(x)|)$ up to polylogarithmic factors. Combining this typical-case rarity with an independent per-site mutation model and optimizing the resulting expected supply rate yields an optimal mutation load $U^\star=O(1)$ and, in particular, an optimal per-site rate
%% \[
%% \mu^\star=\frac{1+o(1)}{n}.
%% \]
%% In this sense, Drake’s rule emerges as an information-theoretic supply optimum under blind local perturbation, complementing fidelity- and stability-based explanations such as the error-threshold viewpoint.





%% We quantify this atypicality using \emph{randomness deficiency} with respect to the mutation-induced model on $H_m(x)$, a standard notion in algorithmic statistics that measures how much rarer (or more structured) an outcome is than a typical draw from the model. Concretely, a mutant is called $\Delta$-exceptional if it is rarer under mutation alone than typical outcomes at the same radius by a margin $\Delta$. This is not a definition of biological fitness, and it is not a claim that real genomes are random. It is a minimal, model-relative notion of being an outlier under blind local perturbation, which is precisely the kind of outcome that can be preferentially retained by selection rather than washed into the typical set of the mutation process.
%% A technical ingredient is the incompressibility principle: in any large discrete space, most elements cannot be described substantially shorter than their explicit specification. For strings of length $n$ over an alphabet of size $q$,
%% \[
%% \Pr\!\left[K(X)\le n\log q - t\right]\le q^{-t},
%% \]
%% for $X$ uniform on $[q]^n$. We use this counting-measure notion of typicality only as a proof device for sharp probability bounds within mutation neighborhoods; it is not an empirical assertion about genomic structure.



%% \subsection*{Why focus on discovery probability?}

%% A natural objection is to ask why evolution should be viewed as optimizing the probability of structure discovery, rather than a more direct quantity such as expected fitness gain~\cite{Lynch2010,SniegowskiRaynes2013,Hershberg2015}. We emphasize that our goal is not to model fitness, nor to assert that natural selection explicitly maximizes the objective we write down. Rather, we use discovery probability to isolate an upstream constraint imposed by mutation itself on the supply of potentially selectable variants.

%% Fitness effects of mutations are inherently unknown \emph{a priori}. From the perspective of the mutation process, there is no access to a fitness landscape. Mutation generates variants blindly, and selection subsequently amplifies or suppresses them. In this setting, one can ask a counterfactual but well-posed question: given a local mutation kernel, which per-site mutation rate maximizes the expected rate at which mutation produces variants that are atypical relative to that kernel. Randomness deficiency relative to the mutation-induced model captures precisely this notion of atypicality: such variants are rare under mutation alone and therefore constitute candidates that selection can preferentially amplify if they confer advantage in a given environment.


%% Finally, it is useful to situate this result relative to classical ``error threshold'' arguments. Eigen-type frameworks constrain mutation from above by requiring that inherited information remain stable under copying errors~\cite{Eigen1971}. Our result is different in kind: it identifies an interior optimum for the upstream \emph{supply} of mutation-atypical variants under blind local perturbations, without invoking a fitness landscape or equilibrium population genetics. The fact that both perspectives emphasize the $U=T=n\mu=O(1)$ regime suggests that Drake's rule may reflect the intersection of two generic pressures: fidelity constraints that limit loss of existing information, and supply constraints that limit how often mutation produces variants sufficiently atypical to be preferentially amplified. In this sense, operating near one mutation per genome per generation can be read as a regime in which both retention and exploration remain nontrivial.


%% This framing connects naturally to classical biology concepts. Population-genetic error-threshold arguments, such as Eigen's, constrain mutation from above by requiring heritable information to remain stable under copying errors~\cite{Eigen1971}. Our result is different in kind: it identifies an \emph{optimal} scaling for the upstream supply of rare variants under blind local perturbations. The fact that both viewpoints emphasize the $1/n$ regime suggests a convergence of two pressures, fidelity constraints that prevent loss of existing information and supply constraints that prevent evolutionary stasis. In this sense, operating near one mutation per genome per generation can be interpreted as a compromise between maintaining inherited structure and generating a steady trickle of mutation-atypical variants on which selection can act.
%% \begin{abstract}
%%   %% Across organisms and viruses, the per-site mutation rate scales inversely with genome length, yielding an approximately constant number of mutations per genome per generation (Drake’s rule). Prevailing explanations invoke biochemical fidelity constraints or population-genetic error-threshold arguments, which primarily bound mutation rates from above and depend on biological implementation details. Here we give a mechanism-independent derivation from algorithmic information theory by treating mutation as a blind generative process upstream of selection. We show that mutation at Hamming radius $m$ induces a canonical two-part code: it selects a mutation-accessible model (the Hamming sphere) and then an index within that model, with description length set by the size of the mutation neighborhood. We formalize evolutionary novelty as positive randomness deficiency relative to this mutation-induced model, capturing the minimal requirement that adaptive candidates be rare under mutation alone. Using an incompressibility method, we prove that for typical genomes the probability that an $m$-mutant achieves deficiency at least $\Delta$ scales as $\Theta(2^{-\Delta}/|H_m|)$ up to polylogarithmic factors. Combining this scaling with a binomial mutation model and optimizing the resulting discovery rate yields an optimal per-site mutation probability $\mu^\star=(1+o(1))/n$. Thus Drake’s rule emerges as a general information-theoretic constraint on structure discovery under local random perturbations, transcending biological detail.
%%   Across organisms and viruses, the per-site mutation rate scales inversely with genome length, yielding an approximately constant number of mutations per genome per generation (Drake’s rule). Existing explanations typically invoke biochemical fidelity constraints or population-genetic error-threshold arguments that constrain mutation rates from above and depend on biological implementation details. Here we provide a complementary, mechanism-independent perspective by treating mutation as a blind generative process that supplies variation upstream of selection. We ask which mutation rates maximize the rate at which mutation produces variants that are atypical relative to the mutation process itself, since only such exceptional variants can serve as candidates for selective amplification without the lineage drifting toward mutational noise. Using combinatorial counting arguments and typical-case bounds, we show that this supply of mutation-atypical variants is maximized when the expected number of mutations per genome per generation is $O(1)$, independent of any biology. Under an independent per-site mutation model, this condition yields a per-site mutation rate scaling as $\mu^\star=\Theta(1/n)$. Drake’s rule therefore emerges as a general information-theoretic necessity on structure discovery under local stochastic perturbations, rather than as a consequence of specific biochemical or population-genetic assumptions.
%% \end{abstract}
