@techreport{Strouse,
abstract = {Lossy compression fundamentally involves a decision about what is relevant and what is not. The information bottleneck (IB) by Tishby, Pereira, and Bialek formalized this notion as an information-theoretic optimization problem and proposed an optimal tradeoff between throwing away as many bits as possible, and selectively keeping those that are most important. Here, we introduce an alternative formulation, the deter-ministic information bottleneck (DIB), that we argue better captures this notion of compression. As suggested by its name, the solution to the DIB problem is a deterministic encoder, as opposed to the stochastic encoder that is optimal under the IB. We then compare the IB and DIB on synthetic data, showing that the IB and DIB perform similarly in terms of the IB cost function, but that the DIB vastly outperforms the IB in terms of the DIB cost function. Moreover, the DIB offered a 1-2 order of magnitude speedup over the IB in our experiments. Our derivation of the DIB also offers a method for continuously interpolating between the soft clustering of the IB and the hard clustering of the DIB.},
author = {Strouse, D J and Schwab, David J},
file = {:home/ishanu/Documents/Mendeley Desktop/Strouse, Schwab/Unknown/Strouse, Schwab - Unknown - The Deterministic Information Bottleneck.pdf:pdf},
mendeley-groups = {Information{\_}bottleneck},
title = {{The Deterministic Information Bottleneck}},
url = {http://www.auai.org/uai2016/proceedings/papers/319.pdf}
}

@inproceedings{Daskalakis,
 author = {Daskalakis, Constantinos and Diakonikolas, Ilias and Servedio, Rocco A.},
 title = {Learning K-modal Distributions via Testing},
 booktitle = {Proceedings of the Twenty-third Annual ACM-SIAM Symposium on Discrete Algorithms},
 series = {SODA '12},
 year = {2012},
 location = {Kyoto, Japan},
 pages = {1371--1385},
 numpages = {15},
 url = {http://dl.acm.org/citation.cfm?id=2095116.2095224},
 acmid = {2095224},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
} 


@article{Gedeon2012,
abstract = {Information Bottleneck-based methods use mutual information as a distortion function in order to extract relevant details about the structure of a complex system by compression. One of the approaches used to generate optimal compressed representations is by annealing a parameter. In this manuscript we present a common framework for the study of annealing in information distortion problems. We identify features that should be common to any annealing optimization problem. The main mathematical tools that we use come from the analysis of dynamical systems in the presence of symmetry (equivariant bifurcation theory). Through the compression problem, we make connections to the world of combinatorial optimization and pattern recognition. The two approaches use very different vocabularies and consider different problems to be “interesting”. We provide an initial link, through the Normalized Cut Problem, where the two disciplines can exchange tools and ideas.},
author = {Gedeon, Tom{\'{a}}{\v{s}} and Parker, Albert E. and Dimitrov, Alexander G.},
doi = {10.3390/e14030456},
file = {:home/ishanu/Documents/Mendeley Desktop/Gedeon, Parker, Dimitrov/Entropy/Gedeon, Parker, Dimitrov - 2012 - The Mathematical Structure of Information Bottleneck Methods.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {bifurcations,information distortion,phase transition,spontaneous symmetry breaking},
mendeley-groups = {Information{\_}bottleneck},
month = {mar},
number = {12},
pages = {456--479},
publisher = {Molecular Diversity Preservation International},
title = {{The Mathematical Structure of Information Bottleneck Methods}},
url = {http://www.mdpi.com/1099-4300/14/3/456/},
volume = {14},
year = {2012}
}
@article{Karl2012,
abstract = {This paper describes a free energy principle that tries to explain the ability of biological systems to resist a natural tendency to disorder. It appeals to circular causality of the sort found in synergetic formulations of self-organization (e.g., the slaving principle) and models of coupled dynamical systems, using nonlinear Fokker Planck equations. Here, circular causality is induced by separating the states of a random dynamical system into external and internal states, where external states are subject to random fluctuations and internal states are not. This reduces the problem to finding some (deterministic) dynamics of the internal states that ensure the system visits a limited number of external states; in other words, the measure of its (random) attracting set, or the Shannon entropy of the external states is small. We motivate a solution using a principle of least action based on variational free energy (from statistical physics) and establish the conditions under which it is formally equivalent to the information bottleneck method. This approach has proved useful in understanding the functional architecture of the brain. The generality of variational free energy minimisation and corresponding information theoretic formulations may speak to interesting applications beyond the neurosciences; e.g., in molecular or evolutionary biology.},
author = {Karl, Friston and Friston},
doi = {10.3390/e14112100},
file = {:home/ishanu/Documents/Mendeley Desktop/Karl, Friston/Entropy/Karl, Friston - 2012 - A Free Energy Principle for Biological Systems.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {Bayesian,ergodicity,free energy,organization,random dynamical system,self,surprise},
mendeley-groups = {Information{\_}bottleneck},
month = {oct},
number = {12},
pages = {2100--2121},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{A Free Energy Principle for Biological Systems}},
url = {http://www.mdpi.com/1099-4300/14/11/2100/},
volume = {14},
year = {2012}
}
@article{Still2014,
abstract = {This paper synthesizes a recent line of work on automated predictive model making inspired by Rate-Distortion theory, in particular by the Information Bottleneck method. Predictive inference is interpreted as a strategy for efficient communication. The relationship to thermodynamic efficiency is discussed. The overall aim of this paper is to explain how this information theoretic approach provides an intuitive, overarching framework for predictive inference.},
author = {Still, Susanne and Susanne},
doi = {10.3390/e16020968},
file = {:home/ishanu/Documents/Mendeley Desktop/Still, Susanne/Entropy/Still, Susanne - 2014 - Information Bottleneck Approach to Predictive Inference.pdf:pdf},
issn = {1099-4300},
journal = {Entropy},
keywords = {computing engines,dynamical systems,equilibrium thermodynamics,far,from,information bottleneck method,predictive inference,thermodynamic efficiency},
mendeley-groups = {Information{\_}bottleneck},
month = {feb},
number = {2},
pages = {968--989},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Information Bottleneck Approach to Predictive Inference}},
url = {http://www.mdpi.com/1099-4300/16/2/968/},
volume = {16},
year = {2014}
}
@article{Pearce2000,
abstract = {When I first started studying epidemiology, ecological studies were briefly discussed as an inexpensive but unreliable method for studying individual level risk factors for disease. For example, rather than go to the time and expense to establish a cohort study or case-control study of fat intake and breast cancer, you could simply use national dietary and cancer incidence data and, with minimal time and expense, show a strong correlation internationally between fat intake and breast cancer. This approach was quite rightly regarded as inadequate and unreliable because of the many additional forms of bias that can occur in such studies compared with studies of individuals within a population. In particular, the “ecological fallacy” can occur in that factors that are associated with national disease rates may not be associated with disease in individuals.1 For example, almost any disease that is associated with affluence and Westernisation has in the past been associated at the national level with sales of television sets, and nowadays is probably associated at the national level with rates of internet use.

Thus, ecological studies were not a good thing to do, and were a relic of the “pre-modern” phase of epidemiology before it became firmly established with a methodologic paradigm based on the theory of randomised controlled trials of individuals. This paradigm, which is very powerful when used appropriately, gave rise to increasingly sophisticated methods of study design and data analysis. In particular, biostatistical methods that were developed for randomised trials involving a single individual level exposure were used to reformulate and make more rigorous the previously ad hoc epidemiological methods of study design and data analysis.2 3 Thus, epidemiology courses have increasingly become restricted to discussing cohort and case-control studies and the methods of data analysis that fit the clinical trial paradigm on which {\ldots}},
author = {Pearce, N},
doi = {10.1136/JECH.54.5.326},
issn = {0143-005X},
journal = {Journal of epidemiology and community health},
mendeley-groups = {Information{\_}bottleneck},
month = {may},
number = {5},
pages = {326--7},
pmid = {10814650},
publisher = {BMJ Publishing Group Ltd},
title = {{The ecological fallacy strikes back.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10814650 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC1731667},
volume = {54},
year = {2000}
}
@article{Koopman1936,
author = {Koopman, B.O. O},
doi = {10.2307/1989758},
file = {:home/ishanu/Documents/Mendeley Desktop/Koopman/Transactions of the American Mathematical Society/Koopman - 1936 - On Distributions Admitting a Sufficient Statistic.pdf:pdf},
issn = {00029947},
journal = {Transactions of the American Mathematical Society},
mendeley-groups = {stationary{\_}ergodic,Information{\_}bottleneck},
month = {may},
number = {3},
pages = {399--409},
publisher = {American Mathematical Society},
title = {{On Distributions Admitting a Sufficient Statistic}},
url = {http://yaroslavvb.com/papers/koopman-on.pdf http://www.ams.org/journals/tran/1936-039-03/S0002-9947-1936-1501854-3/S0002-9947-1936-1501854-3.pdf},
volume = {39},
year = {1936}
}
@article{Aft,
abstract = {We introduce a new, non-parametric and principled, distance based clustering method. This method combines a pairwise based ap-proach with a vector-quantization method which provide a mean-ingful interpretation to the resulting clusters. The idea is based on turning the distance matrix into a Markov process and then examine the decay of mutual-information during the relaxation of this process. The clusters emerge as quasi-stable structures dur-ing this relaxation, and then are extracted using the information bottleneck method. These clusters capture the information about the initial point of the relaxation in the most effective way. The method can cluster data with no geometric or other bias and makes no assumption about the underlying distribution.},
author = {Aft, N and Tishby, Ali and {Oam Slonim}, N},
file = {:home/ishanu/Documents/Mendeley Desktop/Aft, Tishby, Oam Slonim/Unknown/Aft, Tishby, Oam Slonim - Unknown - Data clustering by Markovian relaxation and the Information Bottleneck Method.pdf:pdf},
mendeley-groups = {Information{\_}bottleneck},
title = {{Data clustering by Markovian relaxation and the Information Bottleneck Method}},
url = {https://papers.nips.cc/paper/1896-data-clustering-by-markovian-relaxation-and-the-information-bottleneck-method.pdf}
}
@article{Chechik2005,
abstract = {The problem of extracting the relevant aspects of data was previously addressed through the infor-mation bottleneck (IB) method, through (soft) clustering one variable while preserving information about another -relevance -variable. The current work extends these ideas to obtain continuous rep-resentations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difficult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projec-tion to eigenvectors of the normalized regression matrix $\Sigma$ x|y $\Sigma$ −1 x , which is also the basis obtained in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the " information-curve "), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections.},
author = {Chechik, Gal and Globerson, Amir and Tishby, Naftali and {Weiss YWEISS}, Yair},
file = {:home/ishanu/Documents/Mendeley Desktop/Chechik et al/Journal of Machine Learning Research/Chechik et al. - 2005 - Information Bottleneck for Gaussian Variables.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Gaussian processes,canonical cor-relation analysis,dimensionality reduction,information bottleneck},
mendeley-groups = {stationary{\_}ergodic,Information{\_}bottleneck},
pages = {165--188},
title = {{Information Bottleneck for Gaussian Variables}},
url = {http://www.jmlr.org/papers/volume6/chechik05a/chechik05a.pdf},
volume = {6},
year = {2005}
}
@article{Gilad-Bachrach,
abstract = {A fundamental question in learning theory is the quantifi-cation of the basic tradeoff between the complexity of a model and its predictive accuracy. One valid way of quantifying this tradeoff, known as the " Information Bottleneck " , is to measure both the complexity of the model and its prediction accuracy by using Shannon's mutual informa-tion. In this paper we show that the Information Bottleneck framework answers a well defined and known coding problem and at same time it provides a general relationship between complexity and prediction ac-cuarcy, measured by mutual information. We study the nature of this complexity-accuracy tradeoff and discuss some of its theoretical proper-ties. Furthermore, we present relations to classical information theoretic problems, such as rate-distortion theory, cost-capacity tradeoff and source coding with side information.},
author = {Gilad-Bachrach, Ran and Navot, Amir and Tishby, Naftali},
file = {:home/ishanu/Documents/Mendeley Desktop/Gilad-Bachrach, Navot, Tishby/Unknown/Gilad-Bachrach, Navot, Tishby - Unknown - An Information Theoretic Tradeoff between Complexity and Accuracy.pdf:pdf},
mendeley-groups = {stationary{\_}ergodic,Information{\_}bottleneck},
title = {{An Information Theoretic Tradeoff between Complexity and Accuracy}},
url = {https://www.cs.huji.ac.il/labs/learning/Papers/ib{\_}theory.pdf}
}
@article{Shamir2010,
abstract = {a b s t r a c t The Information Bottleneck is an information theoretic framework that finds concise representations for an 'input' random variable that are as relevant as possible for an 'output' random variable. This framework has been used successfully in various supervised and unsupervised applications. However, its learning theoretic properties and justification remained unclear as it differs from standard learning models in several crucial aspects, primarily its explicit reliance on the joint input–output distribution. In practice, an empirical plug-in estimate of the underlying distribution has been used, so far without any finite sample performance guarantees. In this paper we present several formal results that address these difficulties. We prove several finite sample bounds, which show that the information bottleneck can provide concise representations with good generalization, based on smaller sample sizes than needed to estimate the underlying distribution. The bounds are non-uniform and adaptive to the complexity of the specific model chosen. Based on these results, we also present a preliminary analysis on the possibility of analyzing the information bottleneck method as a learning algorithm in the familiar performance-complexity tradeoff framework. In addition, we formally describe the connection between the information bottleneck and minimal sufficient statistics.},
author = {Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
doi = {10.1016/j.tcs.2010.04.006},
file = {:home/ishanu/Documents/Mendeley Desktop/Shamir, Sabato, Tishby/Theoretical Computer Science/Shamir, Sabato, Tishby - 2010 - Learning and generalization with the information bottleneck(2).pdf:pdf},
journal = {Theoretical Computer Science},
keywords = {Information bottleneck,Information theory,Statistical learning theory,Sufficient statistics},
mendeley-groups = {stationary{\_}ergodic,bioshock{\_}refs,Information{\_}bottleneck},
pages = {2696--2711},
title = {{Learning and generalization with the information bottleneck}},
url = {https://ac.els-cdn.com/S030439751000201X/1-s2.0-S030439751000201X-main.pdf?{\_}tid=3401cf30-f84d-11e7-90ba-00000aab0f01{\&}acdnat=1515839792{\_}ecda5ee7fba70d7bb2d4ce783bb7934f},
volume = {411},
year = {2010}
}
@article{Tishby2000,
abstract = {We define the relevant information in a signal x ∈ X as being the in-formation that this signal provides about another signal y ∈ Y . Examples include the information that face images provide about the names of the peo-ple portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize this problem as that of finding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a 'bottleneck' formed by a limited set of codeword X. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x x) emerges from the joint statistics of X and Y . This approach yields an exact set of self consistent equations for the coding rules X X an X → Y . Solutions to these equations can be found by a convergent re–estimation method that generalizes the Blahut–Arimoto algorithm. Our variational principle pro-vides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
archivePrefix = {arXiv},
arxivId = {arXiv:physics/0004057v1},
author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
eprint = {0004057v1},
file = {:home/ishanu/Documents/Mendeley Desktop/Tishby, Pereira, Bialek/Unknown/Tishby, Pereira, Bialek - 2000 - The information bottleneck method.pdf:pdf},
mendeley-groups = {stationary{\_}ergodic,Information{\_}bottleneck},
primaryClass = {arXiv:physics},
title = {{The information bottleneck method}},
url = {https://arxiv.org/pdf/physics/0004057.pdf},
year = {2000}
}
